{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Markov Model (HMM) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Markov Models (HMMs) are a class of probabilistic graphical model that allow us to predict a sequence of unknown (hidden) variables from a set of observed variables. A simple example of an HMM is predicting the weather (hidden variable) based on the type of clothes that someone wears (observed). An HMM can be viewed as a Bayes Net unrolled through time with observations made at a sequence of time steps being used to predict the best sequence of hidden states.\n",
    "\n",
    "The reason it is called a Hidden Markov Model is because we are constructing an inference model based on the assumptions of a Markov process. The Markov process assumption is simply that the “future is independent of the past given the present”. In other words, assuming we know our present state, we do not need any other historical information to predict the future state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Models\n",
    "\n",
    "Let's talk about the weather Here in Berkeley we have three types of weather: sunny rainy and foggy Let's assume for the moment that the weather lasts all day ie it doesn't change from rainy to sunny in the middle of the day.\n",
    "\n",
    "Weather prediction is all about trying to guess what the weather will be like tomorrow based on a history of observations of weather Let's assume a simplified model of weather prediction we'll collect statistics on what the weather was like today based on what the weather was like yesterday the day before and so forth We want to collect the following probabilities:\n",
    "\n",
    "(1) $$ P(w_n|w_{n-1}, w_{n-2},...,w_{n-3})$$\n",
    "\n",
    "\n",
    "Using expression (1) we can give probabilities of types of weather for tomorrow and the next day using n days of history For example if we knew that the weather for the past three days was sunny sunny foggy in chronological order the probability that tomorrow would be rainy is given by:\n",
    "\n",
    "(2) $$ P (w_4=Rainy|w_3=Foggy, w_2=Sunny, w_1=Sunny)$$\n",
    "\n",
    "\n",
    "Here's the problem: the larger n is the more statistics we must collect. Suppose that `n =5` then we must collect statistics for $ 3^5 = 243 $ past histories Therefore we will make a simplifying assumption called the Markov Assumption:\n",
    "\n",
    "In a sequence $ \\{w_1, w_2,...,w_n\\}$  \n",
    "(3) $$ P(w_n | w_{n-1}, w_{n-2},...,w_1) \\approx P(w_n | w_{n-1}) $$\n",
    "\n",
    "\n",
    "This is called a first order Markov assumption since we say that the probability of an observation at time `n` only depends on the observation at time `n-1`. A secondorder Markov assumption would have the observation at time `n` depend on `n-1` and `n-2`. In general when people talk about Markov assumptions they usually mean first order Markov assumptions; I will use the two terms interchangeably.\n",
    "\n",
    "We can the express the joint probability using the Markov $ assumption^1 $\n",
    "\n",
    "(4) $$ P(w_1,...,w_n) = \\prod_{i=0}^n P(w_i | w_{i-1}) $$\n",
    "\n",
    "\n",
    "So this now has a profound affect on the number of histories that we have to find statistics for -  we now only need $ 3^2 = 9 $ numbers to characterize the probabilities of all of the sequences This assumption may or may not be a valid assumption depending on the situation (in the case of weather, it's probably not valid) but we use these to simplify the situation.\n",
    "\n",
    "So let's arbitrarily pick some numbers for $ P(w_{tomorrow} | w_{today}) $ expressed in Table 1.\n",
    "\n",
    "<i></i>             | <i></i> | <i></i> | Tommorow's|  Weather \n",
    "---------------     | ------- | ------- | ----------| -----------------\n",
    "<i></i>             | <i></i> | Sunny   | Rainy  | Foggy\n",
    "<i></i>             | Sunny   | 0.8     | 0.05   | 0.15\n",
    "__Today's Weather__ | Rainy   | 0.2     | 0.6    | 0.2\n",
    "<i></i>             | Foggy   | 0.2     | 0.3    | 0.5\n",
    "\n",
    "\n",
    "<center>Table 1: Probabilities of Tomorrow's weather based on Today's Weather</center>\n",
    "\n",
    "\n",
    "For first order Markov models we can use these probabilities to draw a probabilistic finite state automaton For the weather domain you would have three states (Sunny, Rainy and Foggy) and every day you would transition to a (possibly) new state based on the probabilities in Table 1 Such an automaton would look like this:\n",
    "\n",
    "![HMM](./images/HMM_Weather.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Markov Models\n",
    "\n",
    "So what makes a Hidden Markov Model? Well suppose you were locked in a room for several days and you were asked about the weather outside The only piece of evidence you have is whether the person who comes into the room carrying your daily meal is carrying an umbrella or not \n",
    "\n",
    "Let's suppose the following probabilities:\n",
    "\n",
    "| <i></i> | Probability of Umbrella\n",
    "| ------- | ------- |\n",
    "| Sunny   | 0.1     |\n",
    "| Rainy   | 0.8     |\n",
    "| Foggy   | 0.3     |\n",
    "\n",
    "\n",
    "Table 2: Probabilities of Seeing an Umbrella Based on the Weather\n",
    "\n",
    "Remember the equation for the weather Markov process before you were locked in the room was\n",
    "\n",
    "(5) $$ P(w_1,...,w_n) = \\prod_{i=0}^n P(w_i | w_{i-1}) $$\n",
    "\n",
    "Now we have to factor in the fact that the actual weather is hidden from you We do that by using Bayes' Rule:\n",
    "\n",
    "(6) $$ P(w_1,...,w_n | u_1,...,u_n) = \\frac{P(u_1,...,u_n | w_1,...,w_n) P(w_1,...,w_n)} {P(u_1,...,u_n)} $$\n",
    "\n",
    "\n",
    "where $u_i$ is true if your caretaker brought an umbrella on day `i`, and false if the caretaker didn't. The probability $ P(w_1,...,w_n)$ is the same as the Markov model from the last section and the probability $P(u_1,...,u_n)$ is the prior probability of seeing a particular sequence of umbrella events (eg {True, False, True}). The probability $P(u_1,...,u_n | w_1,...,w_n)$ can be estimated as $ \\prod_{i=0}^n P(u_i | w_i)$ if you assume that for all `i` given $w_i u_i$ is independent of all $u_j$ and $w_j$  for all $j \\ne i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
