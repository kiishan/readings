{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "\n",
    "Activation functions are an extremely important feature of the artificial neural networks. They basically decide whether a neuron should be activated or not. Whether the information that the neuron is receiving is relevant for the given information or should it be ignored.\n",
    "\n",
    "$$ Y = Activation(\\sum (weight * input) + bias) $$\n",
    "\n",
    "The activation function is the non linear transformation that we do over the input signal. This transformed output is then sen to the next layer of neurons as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Can we do without an activation function?__\n",
    "\n",
    "Now the question which arises is that if the activation function increases the complexity so much, can we do without an activation function?\n",
    "\n",
    "When we do not have the activation function the weights and bias would simply do a linear transformation. A linear equation is simple to solve but is limited in its capacity to solve complex problems. A neural network without an activation function is essentially just a linear regression model. The activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks. We would want our neural networks to work on complicated tasks like language translations and image classifications. Linear transformations would never be able to perform such tasks.\n",
    "\n",
    "Activation functions make the back-propagation possible since the gradients are supplied along with the error to update the weights and biases. Without the differentiable non linear function, this would not be possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sigmoid__\n",
    "\n",
    "Sigmoid is a widely used activation function. It is of the form-\n",
    "\n",
    "f(x)=1/(1+e^-x)\n",
    "\n",
    "This is a smooth function and is continuously differentiable. The biggest advantage that it has over step and linear function is that it is non-linear. This is an incredibly cool feature of the sigmoid function. This essentially means that when I have multiple neurons having sigmoid function as their activation function – the output is non linear as well. The function ranges from 0-1 having an S shape. Let’s take a look at the shape of the curve. The gradient is very high between the values of -3 and 3 but gets much flatter in other regions. How is this of any use?\n",
    "\n",
    "This means that in this range small changes in x would also bring about large changes in the value of Y. So the function essentially tries to push the Y values towards the extremes. This is a very desirable quality when we’re trying to classify the values to a particular class.\n",
    "\n",
    "Let’s take a look at the gradient of the sigmoid function as well.\n",
    "\n",
    "\n",
    "It’s smooth and is dependent on x. This means that during backpropagation we can easily use this function. The error can be backpropagated and the weights can be accordingly updated.\n",
    "\n",
    "Sigmoids are widely used even today but we still have a problems that we need to address. As we saw previously – the function is pretty flat beyond the +3 and -3 region. This means that once the function falls in that region the gradients become very small. This means that the gradient is approaching to zero and the network is not really learning.\n",
    "\n",
    "Another problem that the sigmoid function suffers is that the values only range from 0 to 1. This means that the sigmoid function is not symmetric around the origin and the values received are all positive. So not all times would we desire the values going to the next neuron to be all of the same sign. This can be addressed by scaling the sigmoid function. That’s exactly what happens in the tanh function. let’s read on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tanh__\n",
    "\n",
    "The tanh function is very similar to the sigmoid function. It is actually just a scaled version of the sigmoid function.\n",
    "\n",
    "\n",
    "tanh(x)=2sigmoid(2x)-1\n",
    "\n",
    "It can be directly written as –\n",
    "\n",
    "tanh(x)=2/(1+e^(-2x)) -1\n",
    "\n",
    "Tanh works similar to the sigmoid function but is symmetric over the origin. it ranges from -1 to 1.\n",
    "\n",
    "\n",
    "It basically solves our problem of the values all being of the same sign. All other properties are the same as that of the sigmoid function. It is continuous and differentiable at all points. The function as you can see is non linear so we can easily backpropagate the errors.\n",
    "\n",
    "Let’s have a look at the gradient of the tan h function.\n",
    "\n",
    "\n",
    "The gradient of the tanh function is steeper as compared to the sigmoid function. Our choice of using sigmoid or tanh would basically depend on the requirement of gradient in the problem statement. But similar to the sigmoid function we still have the vanishing gradient problem. The graph of the tanh function is flat and the gradients are very low.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ReLU__\n",
    "\n",
    "The ReLU function is the Rectified linear unit. It is the most widely used activation function. It is defined as-\n",
    "\n",
    "f(x)=max(0,x)\n",
    "It can be graphically represented as-\n",
    "\n",
    "\n",
    "ReLU is the most widely used activation function while designing networks today. First things first, the ReLU function is non linear, which means we can easily backpropagate the errors and have multiple layers of neurons being activated by the ReLU function.\n",
    "\n",
    "The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time. What does this mean ? If you look at the ReLU function if the input is negative it will convert it to zero and the neuron does not get activated. This means that at a time only a few neurons are activated making the network sparse making it efficient and easy for computation.\n",
    "\n",
    "Let’s look at the gradient of the ReLU function.\n",
    "\n",
    "But ReLU also falls a prey to the gradients moving towards zero. If you look at the negative side of the graph, the gradient is zero, which means for activations in that region, the gradient is zero and the weights are not updated during back propagation. This can create dead neurons which never get activated. When we have a problem, we can always engineer a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Leaky ReLU__\n",
    "\n",
    "Leaky ReLU function is nothing but an improved version of the ReLU function. As we saw that for the ReLU function, the gradient is 0 for x<0, which made the neurons die for activations in that region. Leaky ReLU is defined to address this problem. Instead of defining the Relu function as 0 for x less than 0, we define it as a small linear component of x. It can be defined as-\n",
    "\n",
    "f(x)= ax, x<0\n",
    "= x, x>=0\n",
    "What we have done here is that we have simply replaced the horizontal line with a non-zero, non-horizontal line. Here a is a small value like 0.01 or so. It can be represented on the graph as-\n",
    "\n",
    "The main advantage of replacing the horizontal line is to remove the zero gradient. So in this case the gradient of the left side of the graph is non zero and so we would no longer encounter dead neurons in that region. The gradient of the graph would look like –\n",
    "\n",
    "Similar to the Leaky ReLU function, we also have the Parameterised ReLU function. It is defined similar to the Leaky ReLU as –\n",
    "\n",
    "f(x)= ax, x<0\n",
    "= x, x>=0\n",
    "However, in case of a parameterised ReLU function, ‘a‘ is also a trainable parameter. The network also learns the value of ‘a‘ for faster and more optimum convergence. The parametrised ReLU function is used when the leaky ReLU function still fails to solve the problem of dead neurons and the relevant information is not successfully passed to the next layer."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Softmax__  \n",
    "The softmax function is also a type of sigmoid function but is handy when we are trying to handle classification problems. The sigmoid function as we saw earlier was able to handle just two classes. What shall we do when we are trying to handle multiple classes. Just classifying yes or no for a single class would not help then. The softmax function would squeeze the outputs for each class between 0 and 1 and would also divide by the sum of the outputs. This essentially gives the probability of the input being in a particular class. It can be defined as –\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Let’s say for example we have the outputs as-\n",
    "\n",
    "[1.2 , 0.9 , 0.75], When we apply the softmax function we would get [0.42 ,  0.31, 0.27]. So now we can use these as probabilities for the value to be in each class.\n",
    "The softmax function is ideally used in the output layer of the classifier where we are actually trying to attain the probabilities to define the class of each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
